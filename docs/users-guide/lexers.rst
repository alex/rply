Generating Lexers
=================

In order to parse text, you first have to turn that text into individual tokens
with a lexer. Such a lexer can be generated with the
:class:`rply.LexerGenerator`.

Lexers are generated by adding rules to a `LexerGenerator` instance. Such a
rule consists of a name, which will be used as the type of the token generated
with that rule, and a regular expression defining the piece of text to be
matched.

As an example we will attempt to generate a lexer for simple mathematical
expressions::

    lg = LexerGenerator()

    lg.add('NUMBER', r'\d+')

    lg.add('PLUS', r'\+')
    lg.add('MINUS', r'-')

We have no defined rules for numbers, an addition and subtraction operator.
We can now build a lexer and use it::

    >>> l = lg.build()
    >>> for token in l.lex('1+1-1'):
    ...     print(token)
    ...
    Token(NUMBER, '1')
    Token(ADD, '+')
    Token(NUMBER, '1')
    Token(MINUS, '-')
    Token(NUMBER, '1')

This works quite nicely however there is but a small problem::

    >>> for token in l.lex('1 + 1'):
    ...     print(token)
    Token('NUMBER', '1')
    Traceback (most recent call last):
    ...
    rply.errors.LexingError

What happened is that the lexer is able to match the `1` at the beginning of
the string and it yields the correct token for that but afterwards the string
`` + 1`` is left and no rule matches.

While we do want lexing to continue at that stage, we do not care about
whitespace and would like to ignore it. This can be done using
:meth:`~rply.LexerGenerator.ignore`::

    lg.ignore(r'\s+')

This adds a rule which will be ignored by the lexer and not produce any
tokens::

    >>> l = lg.build()
    >>> for token in l.lex('1 + 1'):
    ...     print(token)
    ...
    Token('NUMBER', '1')
    Token('ADD', '+')
    Token('NUMBER', '1')


Lexing Nested Structures
------------------------

Some languages have nested structures that affect the lexer. One example of
such a structure are nested comments. By definition it's not possible to match
such a comment using a regular expression, nevertheless we want to be able to
produce tokens without involving the parser.

This problem is solved by keeping a stack of states in the lexer. States
represent a set of rules and the lexer only uses those rules found in the state
at the top of the stack. We can then change the state by pushing states to or
popping them off the stack, thereby restricting or expanding the possible
tokens the lexer is able to produce.

To go with the nested comments example we would add a new rule that matches
the begining of a comment::

    lg.add('COMMENT_START', r'/\*', transition='push', target='comment')

This adds a rule that matches ``/*``, when encountered it pushes a new state
to the stack called `comment`. Let's define this state::

    comment = lg.add_state('comment')
    comment.add('COMMENT_START', r'/\*', transition='push', target='comment')
    comment.add('COMMENT_END', r'\*/', transition='pop')
    comment.add('COMMENT_TEXT', r'([^/*]|(/(?!\*))|(\*(?!/)))+')

This `comment` state consists of three rules, `COMMENT_START` allows us to
handle nested comments, `COMMENT_END` pops the comment to give back control to
the previous state which is either a comment or our initial state and
`COMMENT_TEXT` matches any combination of characters that does not include the
sequences matched by `COMMENT_START` or `COMMENT_END`.

Using this lexer we can now nicely handle nested comments::

    >>> l = lg.build()
    >>> for token in l.lex('/* this is /* a nested comment */ */'):
    ...     print(token)
    ...
    Token('COMMENT_START', '/*')
    Token('COMMENT_TEXT', ' this is ')
    Token('COMMENT_START', '/*')
    Token('COMMENT_TEXT', ' a nested comment ')
    Token('COMMENT_END', '*/')
    Token('COMMENT_TEXT', ' ')
    Token('COMMENT_END', '*/')
